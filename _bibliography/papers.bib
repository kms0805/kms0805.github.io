---
---
@article{lee2024VlindBenchML,
      title={VLind-Bench: Measuring Language Priors in Large Vision-Language Models}, 
      author={Lee, Kang-il and Kim, Minbeom and Yoon, Seunghyun and Kim, Minsung and Lee, Dongryeol and Koh, Hyukhun and Jung, Kyomin},
      year={2024},
      abstract={Large Vision-Language Models (LVLMs) have demonstrated outstanding performance across various multimodal tasks. However, they suffer from a problem known as language prior, where responses are generated based solely on textual patterns while disregarding image information. Addressing the issue of language prior is crucial, as it can lead to undesirable biases or hallucinations when dealing with images that are out of training distribution. Despite its importance, current methods for accurately measuring language priors in LVLMs are poorly studied. Although existing benchmarks based on counterfactual or out-of-distribution images can partially be used to measure language priors, they fail to disentangle language priors from other confounding factors. To this end, we propose a new benchmark called VLind-Bench, which is the first benchmark specifically designed to measure the language priors, or blindness, of LVLMs. It not only includes tests on counterfactual images to assess language priors but also involves a series of tests to evaluate more basic capabilities such as instance comprehension, visual perception, and commonsense biases. For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language priors, thereby minimizing the influence of other factors on the assessment. The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors. For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language prior, thereby minimizing the influence of other factors on the assessment. The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors.},
      journal={arXiv prerprint},
      code={https://github.com/klee972/VLind-Bench},
      pdf={https://arxiv.org/pdf/2406.08702},
      dataset={https://huggingface.co/datasets/klee972/VLind-Bench},
      preview={VLind.png}
}
@article{lee-etal-2024-Fine,
    title = "Fine-grained Gender Control in Machine Translation with Large Language Models",
    author = "Lee, Minwoo  and
      Koh, Hyukhun  and
      Kim, Minsung  and
      Jung, Kyomin",
    journal = "NAACL 2024",
    month = jun,
    year = "2024",
    selected={true},
    preview=fine.png
  }



@article{Yang2023is,
    title = {MVMR: A New Framework for Evaluating Faithfulness of Video Moment Retrieval against Multiple Distractors},
    author = {Nakyeong Yang and Minsung Kim and Seunghyun Yoon and Joongbo Shin and Kyomin Jung},
    journal = "CIKM 2024",
    month = oct,
    year = "2024",
    selected={true},
    preview=mvmr.png
}

@article{lee-etal-2023-target,
    title = "Target-Agnostic Gender-Aware Contrastive Learning for Mitigating Bias in Multilingual Machine Translation",
    author = "Lee, Minwoo  and
      Koh, Hyukhun  and
      Lee, Kang-il  and
      Zhang, Dongdong  and
      Kim, Minsung  and
      Jung, Kyomin",
    journal = "EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.1046",
    doi = "10.18653/v1/2023.emnlp-main.1046",
    pages = "16825--16839",
    selected={true},
    preview=gacl.jpg
}


